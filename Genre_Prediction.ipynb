{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Importing all the neccessary libraries required \n",
    "import os\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np  # Numpy library for creating and modifying arrays.\n",
    "from keras.layers import Dense, SimpleRNN, GRU, LSTM, Embedding # Import layers from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching action files\n",
    "path_to_Action = '/home/sandy5290/Downloads/Dataset/TrainDataset/Action/'\n",
    "Action_files = [Action for Action in os.listdir(path_to_Action) if Action.endswith('.txt')]\n",
    "\n",
    "#fetching comedy files\n",
    "path_to_Comedy = '/home/sandy5290/Downloads/Dataset/TrainDataset/Comedy/'\n",
    "Comedy_files = [Comedy for Comedy in os.listdir(path_to_Comedy) if Comedy.endswith('.txt')]\n",
    "\n",
    "#fetching Crime files\n",
    "path_to_Crime = '/home/sandy5290/Downloads/Dataset/TrainDataset/Crime/'\n",
    "Crime_files = [Crime for Crime in os.listdir(path_to_Crime) if Crime.endswith('.txt')]\n",
    "\n",
    "#fetching Horror files\n",
    "path_to_Horror = '/home/sandy5290/Downloads/Dataset/TrainDataset/Horror/'\n",
    "Horror_files = [Horror for Horror in os.listdir(path_to_Horror) if Horror.endswith('.txt')]\n",
    "\n",
    "#fetching Musical files\n",
    "path_to_Musical = '/home/sandy5290/Downloads/Dataset/TrainDataset/Musical/'\n",
    "Musical_files = [Musical for Musical in os.listdir(path_to_Musical) if Musical.endswith('.txt')]\n",
    "\n",
    "#fetching Romance files\n",
    "path_to_Romance = '/home/sandy5290/Downloads/Dataset/TrainDataset/Romance/'\n",
    "Romance_files = [Romance for Romance in os.listdir(path_to_Romance) if Romance.endswith('.txt')]\n",
    "\n",
    "#fetching War files\n",
    "path_to_War = '/home/sandy5290/Downloads/Dataset/TrainDataset/War/'\n",
    "War_files = [War for War in os.listdir(path_to_War) if War.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe to store action subtitles\n",
    "action_df = pd.DataFrame(columns=['Subtitle','Genre'])\n",
    "\n",
    "#reading each file and removing nextline tags,punctuations from the file which are not required\n",
    "for index,txt in enumerate(Action_files):\n",
    "    act_file = codecs.open(os.path.join(path_to_Action,txt),encoding = \"ISO-8859-1\")\n",
    "    Sub = act_file.read().replace(\"\\n\",\" \")\n",
    "    Subtitle = Sub.translate(str.maketrans('','',string.punctuation))\n",
    "    Genre = 'Action'\n",
    "    action_df.loc[index] = [Subtitle,Genre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping 3rd index since it contained nly special characters\n",
    "action_df.drop(index=3,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resetting the index\n",
    "action_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeating same for different genre\n",
    "comedy_df = pd.DataFrame(columns=['Subtitle','Genre'])\n",
    "\n",
    "for index,txt in enumerate(Comedy_files):\n",
    "    com_file = codecs.open(os.path.join(path_to_Comedy,txt),encoding = \"ISO-8859-1\")\n",
    "    Sub = com_file.read().replace(\"\\n\",\" \")\n",
    "    Subtitle = Sub.translate(str.maketrans('','',string.punctuation))\n",
    "    Genre = 'Comedy'\n",
    "    comedy_df.loc[index] = [Subtitle,Genre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_df = pd.DataFrame(columns=['Subtitle','Genre'])\n",
    "\n",
    "for index,txt in enumerate(Crime_files):\n",
    "    cri_file = codecs.open(os.path.join(path_to_Crime,txt),encoding = \"ISO-8859-1\")\n",
    "    Sub = cri_file.read().replace(\"\\n\",\" \")\n",
    "    Subtitle = Sub.translate(str.maketrans('','',string.punctuation))\n",
    "    Genre = 'Action'\n",
    "    crime_df.loc[index] = [Subtitle,Genre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "horror_df = pd.DataFrame(columns=['Subtitle','Genre'])\n",
    "\n",
    "for index,txt in enumerate(Horror_files):\n",
    "    hor_file = codecs.open(os.path.join(path_to_Horror,txt),encoding = \"ISO-8859-1\")\n",
    "    Sub = hor_file.read().replace(\"\\n\",\" \")\n",
    "    Subtitle = Sub.translate(str.maketrans('','',string.punctuation))\n",
    "    Genre = 'Horror'\n",
    "    horror_df.loc[index] = [Subtitle,Genre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "musical_df = pd.DataFrame(columns=['Subtitle','Genre'])\n",
    "\n",
    "for index,txt in enumerate(Musical_files):\n",
    "    mus_file = codecs.open(os.path.join(path_to_Musical,txt),encoding = \"ISO-8859-1\")\n",
    "    Sub = mus_file.read().replace(\"\\n\",\" \")\n",
    "    Subtitle = Sub.translate(str.maketrans('','',string.punctuation))\n",
    "    Genre = 'Musical'\n",
    "    musical_df.loc[index] = [Subtitle,Genre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "romance_df = pd.DataFrame(columns=['Subtitle','Genre'])\n",
    "\n",
    "for index,txt in enumerate(Romance_files):\n",
    "    rom_file = codecs.open(os.path.join(path_to_Romance,txt),encoding = \"ISO-8859-1\")\n",
    "    Sub = rom_file.read().replace(\"\\n\",\" \")\n",
    "    Subtitle = Sub.translate(str.maketrans('','',string.punctuation))\n",
    "    Genre = 'Romance'\n",
    "    romance_df.loc[index] = [Subtitle,Genre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "war_df = pd.DataFrame(columns=['Subtitle','Genre'])\n",
    "\n",
    "for index,txt in enumerate(War_files):\n",
    "    war_file = codecs.open(os.path.join(path_to_War,txt),encoding = \"ISO-8859-1\")\n",
    "    Sub = war_file.read().replace(\"\\n\",\" \")\n",
    "    Subtitle = Sub.translate(str.maketrans('','',string.punctuation))\n",
    "    Genre = 'War'\n",
    "    war_df.loc[index] = [Subtitle,Genre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muvuca  Zohar Group This is the story of a squ...</td>\n",
       "      <td>War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cambodia To many Westerners it seemed a paradi...</td>\n",
       "      <td>War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Range me Three to five on the legs Theyre comi...</td>\n",
       "      <td>War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MRS TEASDALE I ASK YOU TO RECONSIDER YES YOUR ...</td>\n",
       "      <td>War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the allied landing in Europe had begun under c...</td>\n",
       "      <td>War</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Subtitle Genre\n",
       "0  Muvuca  Zohar Group This is the story of a squ...   War\n",
       "1  Cambodia To many Westerners it seemed a paradi...   War\n",
       "2  Range me Three to five on the legs Theyre comi...   War\n",
       "3  MRS TEASDALE I ASK YOU TO RECONSIDER YES YOUR ...   War\n",
       "4  the allied landing in Europe had begun under c...   War"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "war_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords are those words which do not give any useful information about the content. \n",
    "#So we need to remove stopwords. We fetch stopwords from built in function\n",
    "stopwords = [set(stopwords.words('english'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending all the dataframes in single dataframe which will include all genre\n",
    "final_data = action_df.append([comedy_df,crime_df,horror_df,musical_df,romance_df,war_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reseting the index\n",
    "final_data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe youre under arrest Maybe so but dont turn ...</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SUNNI Whats going on Are you getting this FITZ...</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>THE GREY A job at the end of the world A salar...</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marcus  My God Brother what have you done Yet ...</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Visit birdhdinfo for more m720p Movies Encoded...</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Subtitle   Genre\n",
       "0  Joe youre under arrest Maybe so but dont turn ...  Action\n",
       "1  SUNNI Whats going on Are you getting this FITZ...  Action\n",
       "2  THE GREY A job at the end of the world A salar...  Action\n",
       "3  Marcus  My God Brother what have you done Yet ...  Action\n",
       "4  Visit birdhdinfo for more m720p Movies Encoded...  Action"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying first 5 rows\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    \"\"\"Tokenizes given string.\n",
    "    :param sentence: string to tokenize\n",
    "    :return: list of string with tokens\n",
    "    \"\"\"\n",
    "    sentence_tokenized  = nltk.tokenize.word_tokenize(sentence.replace(\"\\n\", \" \"))\n",
    "    return [token.lower() for token in sentence_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_puncs_numbers_stop_words(tokens):\n",
    "    \"\"\"Remove punctuations in the words, words including numbers and words in the stop_words list.\n",
    "    :param tokens: list of string\n",
    "    :return: list of string with cleaned version\n",
    "    \"\"\"\n",
    "    tokens          = [token.replace(\"'\", '') for token in tokens]\n",
    "    tokens_cleaned  = [token for token in tokens if token.isalpha() and token not in stopwords]\n",
    "    return tokens_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(tokens):\n",
    "    \"\"\"Stems the tokens with nltk SnowballStemmer\n",
    "    :param tokens: list of string\n",
    "    :return: list of string with words stems\n",
    "    \"\"\"\n",
    "    stemmer         = nltk.SnowballStemmer(language='english')\n",
    "    tokens_stemmed  = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(document):\n",
    "    \"\"\"Calls methods _tokenize, _remove_puncs_numbers_stop_words and _stem respectively.\n",
    "    :param document: string to preprocess\n",
    "    :return: string with processed version\n",
    "    \"\"\"\n",
    "    doc_tokenized   = tokenize(document)\n",
    "    doc_cleaned     = remove_puncs_numbers_stop_words(doc_tokenized)\n",
    "    doc_stemmed     = stem(doc_cleaned)\n",
    "    doc_stemmed_str = ' '.join(doc_stemmed)\n",
    "    return doc_stemmed_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['Subtitle'] = final_data['Subtitle'].apply(lambda x:preprocess_document(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joe your under arrest mayb so but dont turn ar...</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sunni what go on are you get this fitzi whoa w...</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the grey a job at the end of the world a salar...</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>marcus my god brother what have you done yet a...</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visit birdhdinfo for more movi encod by bird t...</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Subtitle   Genre\n",
       "0  joe your under arrest mayb so but dont turn ar...  Action\n",
       "1  sunni what go on are you get this fitzi whoa w...  Action\n",
       "2  the grey a job at the end of the world a salar...  Action\n",
       "3  marcus my god brother what have you done yet a...  Action\n",
       "4  visit birdhdinfo for more movi encod by bird t...  Action"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the tags\n",
    "final_data['Subtitle'] = final_data['Subtitle'].str.replace('nbsp|âª|ltigt|apos|shehe|yã|não|yoï','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching all the words from data\n",
    "tfidf = final_data['Subtitle'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing words that occur in more than 80% documents\n",
    "cv=CountVectorizer(max_df=0.8)\n",
    "word_count_vector=cv.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4828x174298 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4135435 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating idf for all the words/vocab\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing all the vocabs in variable\n",
    "feature_names=cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the functions which will sort words based on tf-idf and fetch top 50 words from each document\n",
    "\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items,topn=50):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which fetches keywords along with tf-idf value\n",
    "def get_keywords(idx):\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([tfidf[idx]]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n;\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items)\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping only those words which had high tf-idf value\n",
    "cleaned_text = pd.DataFrame(columns = ['Cleaned_Subs'])\n",
    "for idx in range(0,final_data.shape[0]):\n",
    "    key_wrds = get_keywords(idx)\n",
    "    Pure = list(key_wrds.keys())\n",
    "    cleaned_text.loc[idx] = [Pure]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting list to string\n",
    "cleaned_text['Cleaned_Subs'] = cleaned_text['Cleaned_Subs'].apply(lambda x : ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the dataframes\n",
    "cleaned_data = final_data.join(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping column which was not required\n",
    "cleaned_data.drop(columns = ['Subtitle'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Cleaned_Subs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Action</td>\n",
       "      <td>burdett sheriff stumpi wheeler carlo consuelo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Action</td>\n",
       "      <td>fitzi holden nora zahir sunni pyramid shadid o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Action</td>\n",
       "      <td>fuck ottway henrick talget flanneri diaz burk ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Action</td>\n",
       "      <td>viktor selen marcus kraven lycan william tani ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Action</td>\n",
       "      <td>loki tesseract stark furi romanoff coulson sel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Genre                                       Cleaned_Subs\n",
       "0  Action  burdett sheriff stumpi wheeler carlo consuelo ...\n",
       "1  Action  fitzi holden nora zahir sunni pyramid shadid o...\n",
       "2  Action  fuck ottway henrick talget flanneri diaz burk ...\n",
       "3  Action  viktor selen marcus kraven lycan william tani ...\n",
       "4  Action  loki tesseract stark furi romanoff coulson sel..."
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final dataframe for model building\n",
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizing the words to get the root form of word\n",
    "#cleaned_data['Cleaned_Subs'] = cleaned_data['Cleaned_Subs'].apply(lambda x:' '.join(lemmatize_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Cleaned_Subs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comedy</td>\n",
       "      <td>améli poulain bretodeau bredoteau dominiqu suz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Horror</td>\n",
       "      <td>leona harrigan aah danni fuck mike willi heine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Romance</td>\n",
       "      <td>hodor euron bran sansa stark ramsay meera grey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Horror</td>\n",
       "      <td>krissi jane evey coupland brian harri joseph d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Musical</td>\n",
       "      <td>hojo sidhu wiii lushang kung chowk chandni fu ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Genre                                       Cleaned_Subs\n",
       "0   Comedy  améli poulain bretodeau bredoteau dominiqu suz...\n",
       "1   Horror  leona harrigan aah danni fuck mike willi heine...\n",
       "2  Romance  hodor euron bran sansa stark ramsay meera grey...\n",
       "3   Horror  krissi jane evey coupland brian harri joseph d...\n",
       "4  Musical  hojo sidhu wiii lushang kung chowk chandni fu ..."
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#resetting the index\n",
    "cleaned_data = cleaned_data.sample(frac=1).reset_index(drop=True)\n",
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_words = 50000\n",
    "seq_len = 100\n",
    "embedding_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting data into train and test/val\n",
    "X_train, X_test, y_train, y_test = train_test_split(cleaned_data['Cleaned_Subs'],cleaned_data['Genre'],stratify = cleaned_data['Genre'] ,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3379, 100), (3379, 100))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "tokenizer = Tokenizer(num_words=max_num_words) #Tokenizer is used to tokenize text\n",
    "tokenizer.fit_on_texts(X_train) #Fit this to our corpus\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(X_train) #'text to sequences converts the text to a list of indices\n",
    "x_train = pad_sequences(x_train, maxlen=100) #pad_sequences makes every sequence a fixed size list by padding with 0s \n",
    "\n",
    "x_test = tokenizer.texts_to_sequences(X_train) \n",
    "x_test = pad_sequences(x_test, maxlen=100)\n",
    "\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Musical', 'War', 'Romance', 'Action', 'Horror', 'Comedy']\n"
     ]
    }
   ],
   "source": [
    "unique_labels = list(y_train.unique())\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Action', 'War', 'Horror', 'Romance', 'Musical', 'Comedy']\n"
     ]
    }
   ],
   "source": [
    "u_lab_tst = list(y_test.unique())\n",
    "print(u_lab_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    " # This convers the labels to one-hot vectors(Dummies)\n",
    "\n",
    "y_train_n = np.array([unique_labels.index(i) for i in y_train]) # Convert the word labels to indeces\n",
    "y_train_n = to_categorical(y_train_n) # Dummify the labels\n",
    "y_test_n = np.array([unique_labels.index(i) for i in y_test])\n",
    "y_test_n = to_categorical(y_test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an LSTM model\n",
    "model = Sequential() # Call Sequential to initialize a network\n",
    "model.add(Embedding(input_dim = max_num_words, \n",
    "                    input_length = seq_len, \n",
    "                    output_dim = embedding_size)) # Add an embedding layer which represents each unique token as a vector\n",
    "model.add(LSTM(10,return_sequences=True)) # Add an LSTM layer\n",
    "model.add(LSTM(10,return_sequences=False))\n",
    "#model.add(LSTM(10, return_sequences=False))\n",
    "model.add(Dense(6, activation='softmax')) # Add an ouput layer. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 200)          10000000  \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100, 10)           8440      \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 66        \n",
      "=================================================================\n",
      "Total params: 10,009,346\n",
      "Trainable params: 10,009,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting learning rate value\n",
    "adam = Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks are useful to monitor val_loss. If val_loss doesn't improve for continuous 5 epochs then it will stop executing.\n",
    "#Indicating that there is no need to run more epochs as the val_loss is not improving\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2534 samples, validate on 845 samples\n",
      "Epoch 1/100\n",
      "2534/2534 [==============================] - 20s 8ms/step - loss: 1.7693 - acc: 0.2723 - val_loss: 1.7168 - val_acc: 0.2947\n",
      "Epoch 2/100\n",
      "2534/2534 [==============================] - 19s 8ms/step - loss: 1.3672 - acc: 0.5320 - val_loss: 1.5223 - val_acc: 0.4331\n",
      "Epoch 3/100\n",
      "2534/2534 [==============================] - 18s 7ms/step - loss: 0.8605 - acc: 0.8129 - val_loss: 1.6351 - val_acc: 0.3621\n",
      "Epoch 4/100\n",
      "2534/2534 [==============================] - 18s 7ms/step - loss: 0.5609 - acc: 0.9175 - val_loss: 1.7355 - val_acc: 0.3870\n",
      "Epoch 5/100\n",
      "2534/2534 [==============================] - 16s 6ms/step - loss: 0.3981 - acc: 0.9428 - val_loss: 1.9113 - val_acc: 0.3870\n",
      "Epoch 6/100\n",
      "2534/2534 [==============================] - 16s 7ms/step - loss: 0.3257 - acc: 0.9479 - val_loss: 2.0079 - val_acc: 0.3929\n",
      "Epoch 7/100\n",
      "2534/2534 [==============================] - 19s 7ms/step - loss: 0.2784 - acc: 0.9487 - val_loss: 2.0795 - val_acc: 0.3917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fceefe7abe0>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mention the optimizer, Loss function and metrics to be computed\n",
    "model.compile(optimizer=adam,                  # 'Adam' is a variant of gradient descent technique\n",
    "              loss='categorical_crossentropy', # categorical_crossentropy for multi-class classification\n",
    "              metrics=['accuracy'])            # These metrics are computed for evaluating and stored in history\n",
    "\n",
    "model.fit(x_train, y_train_n, epochs=100,callbacks = callbacks ,validation_split=0.25,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4828,)\n",
      "(4828, 52056)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "x = vectorizer.fit_transform(cleaned_data.Cleaned_Subs)\n",
    "y = cleaned_data.Genre\n",
    "print(y.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting labels to numeric \n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_cat = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y_cat, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 6\n",
    "input_shape = X_train.shape[1]\n",
    "nb_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() # This initializes a sequential model to which we can keep adding layers.\n",
    "model.add(Dense(75, kernel_initializer='uniform', input_dim = input_shape, activation='relu'))\n",
    "#model.add(Dense(70, kernel_initializer='uniform', input_dim = input_shape, activation='relu'))\n",
    "#model.add(Dense(256, kernel_initializer='uniform', input_dim = input_shape, activation='relu'))# Add a dense layer \n",
    "model.add(Dense(6, kernel_initializer='uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting learning and momentum\n",
    "# Adam is the optimizer which is the state of the art Gradient Descent variation. \n",
    "from keras.optimizers import Adam\n",
    "adam = Adam(lr=0.001)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', # CrossEntropy is the loss function. \n",
    "              optimizer=adam,                  # Mention the optimizer\n",
    "              metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3475 samples, validate on 387 samples\n",
      "Epoch 1/100\n",
      "3475/3475 [==============================] - 8s 2ms/step - loss: 1.7371 - acc: 0.2817 - val_loss: 1.6868 - val_acc: 0.2455\n",
      "Epoch 2/100\n",
      "3475/3475 [==============================] - 7s 2ms/step - loss: 1.3485 - acc: 0.4483 - val_loss: 1.3908 - val_acc: 0.4806\n",
      "Epoch 3/100\n",
      "3475/3475 [==============================] - 7s 2ms/step - loss: 0.7320 - acc: 0.8725 - val_loss: 1.2798 - val_acc: 0.5323\n",
      "Epoch 4/100\n",
      "3475/3475 [==============================] - 7s 2ms/step - loss: 0.3558 - acc: 0.9249 - val_loss: 1.2964 - val_acc: 0.5452\n",
      "Epoch 5/100\n",
      "3475/3475 [==============================] - 7s 2ms/step - loss: 0.2100 - acc: 0.9321 - val_loss: 1.3528 - val_acc: 0.5530\n",
      "Epoch 6/100\n",
      "3475/3475 [==============================] - 7s 2ms/step - loss: 0.1579 - acc: 0.9304 - val_loss: 1.4277 - val_acc: 0.5349\n",
      "Epoch 7/100\n",
      "3475/3475 [==============================] - 7s 2ms/step - loss: 0.1366 - acc: 0.9329 - val_loss: 1.4742 - val_acc: 0.5271\n",
      "Epoch 8/100\n",
      "3475/3475 [==============================] - 7s 2ms/step - loss: 0.1256 - acc: 0.9332 - val_loss: 1.5370 - val_acc: 0.5245\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=nb_epochs,callbacks = callbacks, batch_size=32, validation_split=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
